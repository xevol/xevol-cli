# xevol CLI — Streaming Experience Spec

## Overview

When a user adds a YouTube URL, the CLI should provide a real-time streaming experience showing each phase of processing, then stream the spike content as it's generated by the LLM — all in one continuous terminal session.

## Current Architecture

### Existing Infrastructure (API side)
- **Redis Streams** for real-time events:
  - `transcript:{accountId}:{transcriptionId}:{lang}` — transcription processing events
  - `spike:{spikeId}` — spike generation LLM output chunks
- **SSE endpoint**: `GET /spikes/stream/:spikeId` — streams spike content via Server-Sent Events
  - Supports `Last-Event-ID` for resumption
  - Returns `{ status: "complete", data: content }` if already done
  - Heartbeat every 20s
- **BullMQ jobs**: transcription queue → transcriptionClean queue → spikes queue
- **Stream keys** built from: accountId, transcriptionId, spikeId

### Existing Infrastructure (CLI side)
- `xevol add <url>` — submits URL, polls `/v1/status/:id` every 5s
- `xevol analyze <id>` — POST creates/fetches spike, polls if pending
- No streaming, no SSE consumption

## Proposed Flow

```
$ xevol add https://youtube.com/watch?v=abc --stream --analyze review,facts

⠋ Downloading video...
✔ Downloaded: "How to Build a Startup" (12:34) — Y Combinator
⠋ Transcribing audio...
█████████████░░░░░░░░ 65% transcribing...
✔ Transcription complete

⠋ Generating spike: review...
─── Review ───
The video covers the essential steps for building a startup,
focusing on finding product-market fit before scaling. Key points
include talking to users early, building an MVP in weeks not
months, and measuring retention as the primary metric...
[streaming token by token]
✔ Spike complete: review

⠋ Generating spike: facts...
─── Facts ───
1. 90% of startups fail within the first 3 years
2. Product-market fit is the #1 predictor of success
[streaming token by token]
✔ Spike complete: facts

✔ All done. Job ID: xJ7kM2pQ
  Resume anytime: xevol resume xJ7kM2pQ
```

## Commands

### `xevol add <url> --stream [--analyze <prompts>]`

Full pipeline in one command:
1. Submit URL → get transcription ID
2. Connect to transcription status stream (poll or SSE)
3. Show real-time progress: download → transcribe → clean
4. When transcription completes, auto-kick spikes for each prompt
5. Stream each spike's LLM output to terminal in real-time via SSE
6. Print job summary with resume ID at the end

### `xevol stream <spikeId>`

Stream a specific spike's content (for when you already have a spike ID):
1. Connect to `GET /spikes/stream/:spikeId`
2. If complete, print content immediately
3. If pending/processing, stream SSE chunks to terminal

### `xevol resume <jobId>`

Resume a previous streaming session:
1. Look up job by ID (transcription ID or a composite job ID)
2. Check status of transcription + any associated spikes
3. For completed items, print cached content
4. For in-progress items, reconnect to SSE streams
5. For pending spikes, kick them off and stream

## API Changes Required

### 1. Transcription Status Stream (new or enhanced)

**Option A**: Add SSE endpoint `GET /v1/status/:id/stream`
- Streams transcription status changes via SSE
- Events: `downloading`, `transcribing` (with progress %), `cleaning`, `complete`, `error`
- Supports `Last-Event-ID` for resumption

**Option B**: Enhance existing `/v1/status/:id` with richer status
- Current: returns `{ status: "pending" | "transcript" | "complete" | "error" }`
- Enhanced: add `{ progress: 0.65, phase: "transcribing", phaseDetail: "..." }`
- CLI continues polling but shows better progress

**Recommendation**: Option B first (simpler, works now), Option A later for true real-time.

### 2. Batch Spike Creation (new)

`POST /spikes/batch/:transcriptionId`
- Body: `{ prompts: ["review", "facts", "ideas"], outputLang: "en" }`
- Returns: `{ spikes: [{ spikeId, promptId, status }] }`
- Creates all spikes in parallel, returns IDs for streaming

### 3. Job Tracking (new)

`POST /v1/jobs` — Create a composite job (transcription + spikes)
- Returns: `{ jobId, transcriptionId, spikeIds: [...] }`
- Used for resume functionality

`GET /v1/jobs/:jobId` — Get job status
- Returns all component statuses + resume info

## CLI Implementation Details

### SSE Client

```typescript
// New file: src/lib/sse.ts
// EventSource-like client for Node.js that:
// 1. Connects to SSE endpoint with Authorization header
// 2. Parses SSE events (id, event, data fields)
// 3. Supports Last-Event-ID for resumption
// 4. Handles reconnection with backoff
// 5. Emits typed events to callback

async function* streamSSE(url: string, token: string, lastEventId?: string) {
  // Uses fetch with streaming response body
  // Yields parsed SSE events
}
```

### Progress Display

```typescript
// Enhanced spinner that can show:
// 1. Phase labels (Downloading → Transcribing → Cleaning → Generating spikes)
// 2. Progress bars when percentage is available
// 3. Streaming text output (token by token)
// 4. Multiple concurrent progress indicators
```

### Resume State

```typescript
// Store in ~/.xevol/jobs/<jobId>.json
{
  jobId: "xJ7kM2pQ",
  transcriptionId: "abc123",
  url: "https://youtube.com/...",
  spikes: [
    { spikeId: "s1", promptId: "review", status: "complete", lastEventId: "1234" },
    { spikeId: "s2", promptId: "facts", status: "streaming", lastEventId: "5678" }
  ],
  createdAt: "2026-02-02T...",
}
```

## Phased Implementation

### Phase 1: Enhanced Polling (works with current API)
- Better progress display during `add --wait`
- Stream spike content via existing SSE endpoint (`/spikes/stream/:spikeId`)
- `--analyze` flag on add command (already in progress)
- Job state saved locally for resume

### Phase 2: True Streaming
- SSE endpoint for transcription status
- Batch spike creation endpoint
- CLI SSE client with reconnection
- Token-by-token terminal rendering

### Phase 3: Job Management
- Server-side job tracking
- `xevol resume` command
- `xevol jobs` to list active/recent jobs
- Webhook notifications on completion

## Edge Cases

- **Network disconnect mid-stream**: Resume via `Last-Event-ID` header
- **Spike already complete**: SSE endpoint returns JSON immediately (no stream needed)
- **Rate limiting**: Respect 429s, show "waiting..." with countdown
- **Large transcriptions**: Progress percentage for transcription phase
- **Concurrent streams**: Multiple spikes generating in parallel — interleave or sequential?
  - **Recommendation**: Sequential in terminal (cleaner UX), parallel on server

## Dependencies

- No new npm packages needed for SSE client (use native `fetch` with streaming body)
- `ora` already handles spinners
- `cli-table3` already handles tables
- Consider `chalk` for colored streaming output (already installed)
